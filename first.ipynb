{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_openai langgraph langchain tavily-python --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langsmith --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiply tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import ToolMessage, BaseMessage, HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers together.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "# def model, bind tools\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, temperature=0)\n",
    "model_with_tools = model.bind(tools=[convert_to_openai_tool(multiply)])\n",
    "\n",
    "# create graph\n",
    "graph = MessageGraph()\n",
    "\n",
    "# model invoker\n",
    "def invoke_model(state: List[BaseMessage]):\n",
    "    return model_with_tools.invoke(state)\n",
    "\n",
    "# entry point node\n",
    "graph.add_node(\"oracle\", invoke_model)\n",
    "\n",
    "# \n",
    "def invoke_tool(state: List[BaseMessage]):\n",
    "    tool_calls = state[-1].additional_kwargs.get(\"tool_calls\", [])\n",
    "    multiply_call = None\n",
    "\n",
    "    for tool_call in tool_calls:\n",
    "        if tool_call.get(\"function\").get(\"name\") == \"multiply\":\n",
    "            multiply_call = tool_call\n",
    "\n",
    "    if multiply_call is None:\n",
    "        raise Exception(\"No adder input found\")\n",
    "\n",
    "    res = multiply.invoke(\n",
    "        json.loads(multiply_call.get(\"function\").get(\"arguments\"))\n",
    "    )\n",
    "\n",
    "    return ToolMessage(\n",
    "        tool_call_id = multiply_call.get(\"id\"),\n",
    "        content=res\n",
    "    )\n",
    "\n",
    "graph.add_node(\"multiply\", invoke_tool)\n",
    "graph.add_edge(\"multiply\", END)\n",
    "graph.set_entry_point(\"oracle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state: List[BaseMessage]):\n",
    "    tool_calls = state[-1].additional_kwargs.get(\"tool_calls\", [])\n",
    "    if len(tool_calls):\n",
    "        return \"multiply\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "graph.add_conditional_edges(\"oracle\", router, {\n",
    "    \"multiply\": \"multiply\",\n",
    "    \"end\": END,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56088\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "runnable = graph.compile()\n",
    "\n",
    "messages = runnable.invoke(HumanMessage(\"What is 123 * 456?\"))\n",
    "\n",
    "print(messages[-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAVILY_API_KEY = \"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\"\n",
    "LANGCHAIN_API_KEY = \"ls__02b4670bcc97473595c092908b1376df\"\n",
    "LANGCHAIN_TRACING_V2 = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = LANGCHAIN_TRACING_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'format_tools_openai_function' from 'langchain.tools.render' (/Users/brendanmclaughlin/anaconda3/envs/cs224n/lib/python3.11/site-packages/langchain/tools/render.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprebuilt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToolExecutor\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrender\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m format_tools_openai_function\n\u001b[1;32m      6\u001b[0m tools \u001b[38;5;241m=\u001b[39m [TavilySearchResults(max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m      7\u001b[0m tool_executor \u001b[38;5;241m=\u001b[39m ToolExecutor(tools)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'format_tools_openai_function' from 'langchain.tools.render' (/Users/brendanmclaughlin/anaconda3/envs/cs224n/lib/python3.11/site-packages/langchain/tools/render.py)"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools.render import format_tools_openai_function\n",
    "\n",
    "tools = [TavilySearchResults(max_results=1)]\n",
    "tool_executor = ToolExecutor(tools)\n",
    "model = ChatOpenAI(temperature=0, streaming=True)\n",
    "\n",
    "functions = [format_tools_openai_function(t) for t in tools]\n",
    "model = model.bind(tools=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolInvocation\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage\n",
    "\n",
    "# function that determines whether or not to continue\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # if no function to call we're finished\n",
    "    if \"function_call\" not in last_message.additional_kwargs:\n",
    "        return \"end\"\n",
    "    # otherwse continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "# the function that calls the model\n",
    "def call_model(state):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    # return a list bc it will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# define function to execute tools\n",
    "def call_tool(state):\n",
    "    messages = state['messages']\n",
    "    # based on the continue condition we know that the last message has a function call\n",
    "    last_message = messages[-1]\n",
    "    # construct tool invocation from function call\n",
    "    action = ToolsInvocation(\n",
    "        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "        tool_input=json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"])\n",
    "    )\n",
    "    # call tools executor and get back response\n",
    "    response = tool_executor.invoke([action])\n",
    "    # create fuction message\n",
    "    function_message = FunctionMessage(content=str(response), name=action.tool)\n",
    "    # return a list bc it will get added to the existing list\n",
    "    return {\"messages\": [function_message]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", call_tool)\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    # define start node which means there are edges taken after the `agent` node is called\n",
    "    \"agent\", \n",
    "    # next, pass in the function that will determine which node is next\n",
    "    should_continue,\n",
    "    # Finally we pass in a mapping.\n",
    "    # The keys are strings, and the values are other nodes.\n",
    "    # END is a special node marking that the graph should finish.\n",
    "    # What will happen is we will call `should_continue`, and then the output of that\n",
    "    # will be matched against the keys in this mapping.\n",
    "    # Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"action\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# we now add a normal edge from the tool to agent \n",
    "# this means that after `tools` is called, we will call `agent` again\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the weather in SF'),\n",
       "  AIMessage(content='Currently, the weather in San Francisco is partly cloudy with a temperature of 62°F (17°C).', response_metadata={'finish_reason': 'stop'}, id='run-fdf7a2e9-cd7b-4868-8f43-0af08909e13d-0')]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"what is the weather in SF\")]}\n",
    "app.invoke(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
